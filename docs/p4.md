# Project 4: File Systems

## Preliminaries

>Fill in your name and email address.

Rundong Yang <3096842671@qq.com>

>If you have any preliminary comments on your submission, notes for the TAs, please give them here.

We claim to be able to get **89.4** points **without vm**, and **98.1** points **with vm**, 
**so plz build and run tests with vm**.


>Please cite any offline or online sources you consulted while preparing your submission, other than the Pintos documentation, course text, lecture notes, and course staff.

I have consulted the implementation of the file system in [xv6](https://github.com/mit-pdos/xv6-riscv/blob/riscv/kernel/fs.h), especially its inode structure. 
I also refer to [bustub-pj1](https://15445.courses.cs.cmu.edu/fall2023/project1/) because it offers us inspiration about buffer cache's API and possible implementation.

## Indexed And Extensible Files

#### DATA STRUCTURES

>A1: Copy here the declaration of each new or changed struct or struct member, global or static variable, typedef, or enumeration.  Identify the purpose of each in 25 words or less.

In [filesys/inode.c](../src/filesys/inode.c):
```c
/** On-disk inode.
   Must be exactly BLOCK_SECTOR_SIZE bytes long. */
struct inode_disk
  {
    short type;           /**< Type of file */
    short nlink;          /**< Number of links to inode */
    off_t size;           /**< Size of file (bytes) */
    int addrs[125];       /**< Data block addresses */
    unsigned magic;       /**< Magic number */
  };

/**< size of sum of direct sectors */
#define DIRECT_SIZE (123 * BLOCK_SECTOR_SIZE)

/**< size of single indirect blocks */
#define SINGLE_INDIR_SIZE (128 * BLOCK_SECTOR_SIZE)

/**< size of doubly indirect blocks */
#define DOUBLY_INDIR_SIZE (128 * SINGLE_INDIR_SIZE)

/**< maximum size of file */
#define MAXFILE (DIRECT_SIZE + SINGLE_INDIR_SIZE + DOUBLY_INDIR_SIZE)

/** In-memory inode. */
struct inode 
  {
    struct list_elem elem;              /**< Element in inode list. */
    block_sector_t sector;              /**< Sector number of disk location. */
    int open_cnt;                       /**< Number of openers. */
    bool removed;                       /**< True if deleted, false otherwise. */
    int deny_write_cnt;                 /**< 0: writes ok, >0: deny writes. */
    struct inode_disk *data;            /**< Inode content. */
    struct lock lk;                     /**< inode lock */
  };
```
In memory and on disk inode structure.

> A2: What is the maximum size of a file supported by your inode structure?  Show your work.

The maximum size of a file is computed when all direct, singly indirect and doubly indirect sectors are full.
In this case, we have a total of $ d+sid + did = 123\times 512 + 128\times 512 + 128\times\ 128\times 512 = 8517120$ bytes.

#### SYNCHRONIZATION

>A3: Explain how your code avoids a race if two processes attempt to
>extend a file at the same time.

The backup of a file object is an inode. We have a lock for each inode, hence race condition is avoided.


>A4: Suppose processes A and B both have file F open, both
>positioned at end-of-file.  If A reads and B writes F at the same
>time, A may read all, part, or none of what B writes.  However, A
>may not read data other than what B writes, e.g. if B writes
>nonzero data, A is not allowed to see all zeros.  Explain how your
>code avoids this race.

Our lock is acquired  on an inode-level, hence only one process can see the contents of inode
each time. So race is avoided.

>A5: Explain how your synchronization design provides "fairness".
>File access is "fair" if readers cannot indefinitely block writers
>or vice versa.  That is, many processes reading from a file cannot
>prevent forever another process from writing the file, and many
>processes writing to a file cannot prevent another process forever
>from reading the file.

Well, fairness is NOT guaranteed by our implementation. We assume that a less finer-grained 
lock will make it easier to avoid synchroization problems. 

#### RATIONALE

>A6: Is your inode structure a multilevel index?  If so, why did you
>choose this particular combination of direct, indirect, and doubly
>indirect blocks?  If not, why did you choose an alternative inode
>structure, and what advantages and disadvantages does your
>structure have, compared to a multilevel index?

Our inode is multilevel index. We have a singly indirect block plus a doubly indirect block. This 
enables us to create files larger than 8MB.

## Subdirectories

#### DATA STRUCTURES

>B1: Copy here the declaration of each new or changed struct or struct member, global or static variable, typedef, or enumeration.  Identify the purpose of each in 25 words or less.

(void)

#### ALGORITHMS

>B2: Describe your code for traversing a user-specified path.  How
>do traversals of absolute and relative paths differ?

A recursion algorithm is very good for path traversing. The input is the starting inode and
rest of the path. At each recursion, we go an inode-deeper, we stop recursion with path name
is null. This algorithm is implemented in `filesys_walk` and `filesys_leave` 
in [filesys/filesys.c](../src/filesys/filesys.c). 

Traversals of absolute and relative paths differ in that they start from different
points.

#### SYNCHRONIZATION

>B3: How do you prevent races on directory entries?  For example,
>only one of two simultaneous attempts to remove a single file
>should succeed, as should only one of two simultaneous attempts to
>create a file with the same name, and so on.

Since we use inode to represent a directory, race condition is avoided for each access
to an inode will acquire the lock on it.

>B4: Does your implementation allow a directory to be removed if it
>is open by a process or if it is in use as a process's current
>working directory?  If so, what happens to that process's future
>file system operations?  If not, how do you prevent it?

Deletion of an opening directory is allowed. The directory will be physically removed only 
if its open count reaches zero. Hence, a deletion will not crash programs that is operating 
in the directory.

#### RATIONALE

>B5: Explain why you chose to represent the current directory of a
>process the way you did.

Using a sector to store and represent current directory of a proces is sufficient and 
fixed-length.

## Buffer Cache

#### DATA STRUCTURES

>C1: Copy here the declaration of each new or changed struct or struct member, global or static variable, typedef, or enumeration.  Identify the purpose of each in 25 words or less.

In [filesys/bio.c](../src/filesys/bio.c):
```c
/**< metadata of a buffer cache. */
struct buffer_meta 
  {
    uint64_t timestamp;   /**< timestamp, only with non-zero is valid */
    short    dirty;       /**< 1 if page is modified */
    uint16_t pin_cnt;     /**< Pin count */
    block_sector_t sec;   /**< Sector number */
  };

/**< Buffer pool(can hold 48 sectors at once) */
static char buffer_pool[48][BLOCK_SECTOR_SIZE];

/**< Buffer cache base pointer */
static const char *bio_base = (char *)buffer_pool;

/**< Buffer cache bound(end) pointer */
static const char *bio_end = (char *)buffer_pool + sizeof (buffer_pool);

/**< Metadata of 32 caches */
static struct buffer_meta bmeta[BIO_CACHE];

/**< Lock for the entire buffer pool */
static struct lock bplock;
```

The data structures and static variables is self-explained or explained by its comments, hence
no more explanation is provided.

#### ALGORITHMS

>C2: Describe how your cache replacement algorithm chooses a cache
>block to evict.

This algorithm is implemented in `bio_fetch`(in [filesys/bio.c](../src/filesys/bio.c)).
We use LRU replacement policy to choose the block if buffer cache is full, otherwise we choose
an unused block.

>C3: Describe your implementation of write-behind.

We start a "flushing" thread that sleeps for 100 timer ticks(1 second) then flush all dirty pages
to disk. We also flush all dirty pages on shutdown.

>C4: Describe your implementation of read-ahead.

This functionality is not implemented(**TODO**). 

#### SYNCHRONIZATION

>C5: When one process is actively reading or writing data in a
>buffer cache block, how are other processes prevented from evicting
>that block?

We impelement a pin-unpin mechanism to address such situation, i.e. we manage a "pin count"
for each buffered block. It is guaranteed by our eviction policy that a pinned block will
never be evicted. 

Any processes that is using a block will have to "pin" that block so that 
it will not be evicted. The drawback of this mechanism is that user of buffer cache need
to unpin the block on finish, else buffer cache will soon be full and unusable.

>C6: During the eviction of a block from the cache, how are other
>processes prevented from attempting to access the block?

We impelement a pin-unpin mechanism to address such situation, i.e. we manage a "pin count"
for each buffered block. It is guaranteed by our eviction policy that a pinned block will
never be evicted. 

Any processes that is using a block will have to "pin" that block so that 
it will not be evicted. The drawback of this mechanism is that user of buffer cache need
to unpin the block on finish, else buffer cache will soon be full and unusable.

#### RATIONALE

>C7: Describe a file workload likely to benefit from buffer caching,
>and workloads likely to benefit from read-ahead and write-behind.

Since LRU-replacement policy is implemented, if a short number of sectors is frequently 
accessed, then buffer caching will be very useful. 